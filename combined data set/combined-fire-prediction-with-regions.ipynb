{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras as keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from utils import run_all_regressors\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing and EDA(Exploratory data analysis)\n",
    "Data Cleaning and filtering data which has firesize <5000 as number of small fires are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def akReadDf():\n",
    "    # Reading the combined CSV files\n",
    "    df = pd.read_csv('Wildfire.csv')\n",
    "    df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1','disc_date_final','cont_date_final','cont_clean_date','putout_time'])\n",
    "    df['disc_clean_date'] = pd.to_datetime(df['disc_clean_date'], format='%m/%d/%Y')\n",
    "\n",
    "    #Get rid of outliers - fires of size larger than 5000 acres, and there are large number of small fires and other very less number are having the high \n",
    "    # area of fires, because of which the deviation is very high\n",
    "    df = df.loc[df['fire_size'] < 5000]\n",
    "    df.columns\n",
    "\n",
    "    ################\n",
    "    df['Vegetation'] = df['Vegetation'].astype('category')\n",
    "    df['Cause'] = df['stat_cause_descr'].astype('category')\n",
    "\n",
    "    df = pd.get_dummies(df,prefix=['Vegetation'], columns = ['Vegetation'], drop_first=True)\n",
    "    df = pd.get_dummies(df,prefix=['Cause'], columns = ['stat_cause_descr'], drop_first=True)\n",
    "\n",
    "    ################\n",
    "    df_numerics_only = df.select_dtypes(include=np.number)\n",
    "\n",
    "    corr = df_numerics_only.corr()\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(220, 20, n=200),\n",
    "        square=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right'\n",
    "    );\n",
    "    sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "    ################\n",
    "    #Dealing with missing data\n",
    "    print(len(df))\n",
    "\n",
    "    # drop columns where weather_file is missing in the data, as it wont have the weather situation at that time, so its where ever data is \n",
    "    #missing we can remove those rows as it wont be useful\n",
    "    index = df[df['weather_file'] == 'File Not Found'].index\n",
    "    df.drop(index, inplace = True)\n",
    "    print(len(df))\n",
    "\n",
    "\n",
    "    ################\n",
    "    # Weather data has a lot of 0 and values some of which may be missing values,\n",
    "    # Mark '0' values in weather columns as Na (to see how many there are) \n",
    "    # As 0 wont add any value to the data, we are converting to NA and then removing them which will make data set\n",
    "    subset0 = ['Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont']\n",
    "    df[subset0] = df[subset0].replace({0:np.nan, '0':np.nan})\n",
    "    print(len(df))\n",
    "\n",
    "    # Mark '-1' as missing\n",
    "    subset_neg1 = ['Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont']\n",
    "    df[subset_neg1] = df[subset_neg1].replace({-1:np.nan})\n",
    "\n",
    "    # Drop observations where all weather columns are 0\n",
    "    df = df.dropna(how='all',\n",
    "                        subset=['Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont',])\n",
    "    print(len(df))\n",
    "    # This leaves us with 38,689 observations  +/- 3,000  to work with (originally we had 50,000)\n",
    "\n",
    "    ################\n",
    "    # fill the 'pre' columns temp wind and humidity with mean values\n",
    "    subset_fill_mean = ['Temp_pre_30','Temp_pre_15','Temp_pre_7', 'Wind_pre_30','Wind_pre_15','Wind_pre_7', 'Hum_pre_30', 'Hum_pre_15','Hum_pre_7']\n",
    "    df[subset_fill_mean] = df[subset_fill_mean].fillna(df[subset_fill_mean].mean())\n",
    "\n",
    "    # Fill NAs in the date of fire containment based on mean values from previous days\n",
    "    for col in ['Temp','Wind','Hum']:\n",
    "        df[f'{col}_cont'] = df.apply(\n",
    "            lambda row: (row[f'{col}_pre_7']+row[f'{col}_pre_15']+row[f'{col}_pre_30'])/3 if np.isnan(row[f'{col}_cont']) else row[f'{col}_cont'],\n",
    "            axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing region 1 dataframe...\n",
      "Grabbing region 2 dataframe...\n",
      "Grabbing region 3 dataframe...\n",
      "Grabbing region 4 dataframe...\n",
      "Grabbing region 5 dataframe...\n",
      "Grabbing region 6 dataframe...\n",
      "Grabbing region 8 dataframe...\n",
      "Grabbing region 9 dataframe...\n",
      "Grabbing region 10 dataframe...\n"
     ]
    }
   ],
   "source": [
    "from loadDFRegion import getDF\n",
    "df,dfRegionList = getDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445578 1445578\n"
     ]
    }
   ],
   "source": [
    "#Sanity Check !!\n",
    "sum1=0\n",
    "ct=1\n",
    "for dftemp in dfRegionList:\n",
    "    if(ct>=7):\n",
    "        ct+=1\n",
    "    #print(ct, dftemp.size)\n",
    "    sum1+= dftemp.size\n",
    "print(df.size, sum1)\n",
    "assert(sum1==df.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentCreation_FireSizePrediction(df,yTarget):\n",
    "    \"\"\"Function return all experiments splitting data by yTarget name: \n",
    "        regression - fire_size\n",
    "        classification - fire_cause\n",
    "    \"\"\"\n",
    "    \n",
    "    # Experiment 1 \n",
    "    \"\"\"\n",
    "    - which will select all teh available  features from the dataset\n",
    "    -Features included - variables related to Vegetation,Temperature, Humidity, Wind, Precipitation, cause of  fire, longitude and latitude\n",
    "    - we have 34 variables  for x-variables  to which we are gonna target one y-variable which is fire_size\n",
    "    - selecting features and target variables\"\"\"\n",
    "    X1 = df[['Vegetation','remoteness','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','stat_cause_desc', 'longitude']]\n",
    "    #X1 = df[['Vegetation_4','remoteness', 'Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "    # X1 = df[['Vegetation_4','Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "\n",
    "    y = df[yTarget] \n",
    "\n",
    "    #train test split\n",
    "    X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "    df1 = [X1_train, X1_test, y_train, y_test]\n",
    "    #######################\n",
    "    #Experiment type 2 \n",
    "    \"\"\"-Include only long, lat, vegetation, cause and pre- weather data, without cont\n",
    "    - which is the data set where I removed the variables  on which the fire is  containining on the day\n",
    "    - removed 4 variables\n",
    "    - selecting features and target variables\"\"\"\n",
    "    X2 = df[['Vegetation','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Wind_pre_30','Wind_pre_15','Wind_pre_7','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Prec_pre_30','Prec_pre_15','Prec_pre_7','stat_cause_desc', 'longitude']]\n",
    "    #X2 = df[['Vegetation_4','Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Wind_pre_30','Wind_pre_15','Wind_pre_7','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Prec_pre_30','Prec_pre_15','Prec_pre_7','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "    y = df[yTarget]\n",
    "\n",
    "    #train test split\n",
    "    X2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
    "    df2 = [X2_train, X2_test, y_train, y_test]\n",
    "    ########################\n",
    "    #Experiment 3 \n",
    "    #- Including only lat, long and weather pre- data\n",
    "    #When I have done the feature importance, I got to know that the cause and vegetation is not that important, so here we removed the 2 \n",
    "    #  selecting features and target variables\n",
    "    X3 = df[['latitude','longitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Wind_pre_30','Wind_pre_15','Wind_pre_7','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Prec_pre_30','Prec_pre_15','Prec_pre_7']]\n",
    "    y = df[yTarget]\n",
    "\n",
    "    #train test split\n",
    "    X3_train, X3_test, y_train, y_test = train_test_split(X3, y, test_size=0.2, random_state=42)\n",
    "    df3 = [X3_train, X3_test, y_train, y_test]\n",
    "    \n",
    "    ########################\n",
    "    #Experiment 4 \n",
    "    #with experiment 1 data with normalization\n",
    "    # have done the minMax normalization for the experiment 1 data frame.\n",
    "    df_4 = df[['Vegetation','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','stat_cause_desc', 'longitude']]\n",
    "    names = df_4.columns\n",
    "\n",
    "    # normalizing data\n",
    "    df_4 = preprocessing.normalize(df_4)\n",
    "    scaled_df = pd.DataFrame(df_4, columns=names)\n",
    "\n",
    "    #train test split\n",
    "    X4_train, X4_test, y_train, y_test = train_test_split(scaled_df, y, test_size=0.2, random_state=42)\n",
    "    df4 = [X4_train, X4_test, y_train, y_test]\n",
    "    \n",
    "    return [df1,df2,df3,df4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running all regression models on all experiments for Prediction on Fire Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "____Running all experiments for Region 1____\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "\n",
      "--------------Experiment 1--------------\n",
      "17241 4326 821 206\n",
      "Running RandomForestRegressor\n",
      "Score on training data: 0.980933163326186\n",
      "Score on testing data: 0.8617670864622274\n",
      "Mean Absolute Error:  0.03928680008848228\n",
      "R Squared:  0.8617670864622274\n",
      "\n",
      "Running GradientBoostingRegressor\n",
      "Score on training data: 0.9743691746910226\n",
      "Score on testing data: 0.8636906263205764\n",
      "Mean Absolute Error:  0.03799259763637053\n",
      "R Squared:  0.8636906263205764\n",
      "\n",
      "Running DecisionTreeRegressor\n",
      "Score on training data: 0.9999999760149556\n",
      "Score on testing data: 0.8412387659327913\n",
      "Mean Absolute Error:  0.034719173288736235\n",
      "R Squared:  0.8412387659327913\n",
      "\n",
      "Running ExtraTreesRegressor\n",
      "Score on training data: 0.9999999760149556\n",
      "Score on testing data: 0.8628135154455685\n",
      "Mean Absolute Error:  0.037727878844816966\n",
      "R Squared:  0.8628135154455685\n",
      "\n",
      "Running SVR\n",
      "Score on training data: 0.023669598559045824\n",
      "Score on testing data: 0.006796051146053861\n",
      "Mean Absolute Error:  0.16133878041211616\n",
      "R Squared:  0.006796051146053861\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "\n",
      "____Running all experiments for Region 2____\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "\n",
      "--------------Experiment 1--------------\n",
      "29652 7434 1412 354\n",
      "Running RandomForestRegressor\n",
      "Score on training data: 0.9597039036800453\n",
      "Score on testing data: 0.6340133468897913\n",
      "Mean Absolute Error:  0.047931957589783185\n",
      "R Squared:  0.6340133468897913\n",
      "\n",
      "Running GradientBoostingRegressor\n",
      "Score on training data: 0.9051818852303684\n",
      "Score on testing data: 0.6813767095067871\n",
      "Mean Absolute Error:  0.04438356380129165\n",
      "R Squared:  0.6813767095067871\n",
      "\n",
      "Running DecisionTreeRegressor\n",
      "Score on training data: 1.0\n",
      "Score on testing data: 0.2274231220671269\n",
      "Mean Absolute Error:  0.05425349231036863\n",
      "R Squared:  0.2274231220671269\n",
      "\n",
      "Running ExtraTreesRegressor\n",
      "Score on training data: 1.0\n",
      "Score on testing data: 0.8012340427353534\n",
      "Mean Absolute Error:  0.03532165321106675\n",
      "R Squared:  0.8012340427353534\n",
      "\n",
      "Running SVR\n",
      "Score on training data: -0.023215636280210505\n",
      "Score on testing data: -0.04681376121657599\n",
      "Mean Absolute Error:  0.13077681003132435\n",
      "R Squared:  -0.04681376121657599\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "\n",
      "____Running all experiments for Region 3____\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "\n",
      "--------------Experiment 1--------------\n",
      "16380 4095 780 195\n",
      "Running RandomForestRegressor\n",
      "Score on training data: 0.9805532314976432\n",
      "Score on testing data: 0.8889601890435052\n",
      "Mean Absolute Error:  0.03360630287063714\n",
      "R Squared:  0.8889601890435052\n",
      "\n",
      "Running GradientBoostingRegressor\n",
      "Score on training data: 0.990398266838978\n",
      "Score on testing data: 0.8870928087952457\n",
      "Mean Absolute Error:  0.030512417713693424\n",
      "R Squared:  0.8870928087952457\n",
      "\n",
      "Running DecisionTreeRegressor\n",
      "Score on training data: 0.9999999999786066\n",
      "Score on testing data: 0.7995523538927809\n",
      "Mean Absolute Error:  0.03887072060293905\n",
      "R Squared:  0.7995523538927809\n",
      "\n",
      "Running ExtraTreesRegressor\n",
      "Score on training data: 0.9999999999786066\n",
      "Score on testing data: 0.9035353069009564\n",
      "Mean Absolute Error:  0.032052767547624685\n",
      "R Squared:  0.9035353069009564\n",
      "\n",
      "Running SVR\n",
      "Score on training data: 0.007271681249555018\n",
      "Score on testing data: -0.015003288967290862\n",
      "Mean Absolute Error:  0.15552859354597787\n",
      "R Squared:  -0.015003288967290862\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "\n",
      "____Running all experiments for Region 4____\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "\n",
      "--------------Experiment 1--------------\n",
      "24885 6237 1185 297\n",
      "Running RandomForestRegressor\n",
      "Score on training data: 0.9766259651652844\n",
      "Score on testing data: 0.8090798382905457\n",
      "Mean Absolute Error:  0.07526670434812652\n",
      "R Squared:  0.8090798382905457\n",
      "\n",
      "Running GradientBoostingRegressor\n",
      "Score on training data: 0.931085068681722\n",
      "Score on testing data: 0.7887370759564041\n",
      "Mean Absolute Error:  0.07979421942479534\n",
      "R Squared:  0.7887370759564041\n",
      "\n",
      "Running DecisionTreeRegressor\n",
      "Score on training data: 1.0\n",
      "Score on testing data: 0.6924600326351475\n",
      "Mean Absolute Error:  0.08348200531607694\n",
      "R Squared:  0.6924600326351475\n",
      "\n",
      "Running ExtraTreesRegressor\n",
      "Score on training data: 1.0\n",
      "Score on testing data: 0.810282885406462\n",
      "Mean Absolute Error:  0.07564197322736459\n",
      "R Squared:  0.810282885406462\n",
      "\n",
      "Running SVR\n",
      "Score on training data: -0.041617571247722474\n",
      "Score on testing data: -0.08033336420138082\n",
      "Mean Absolute Error:  0.2309731696055511\n",
      "R Squared:  -0.08033336420138082\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "\n",
      "____Running all experiments for Region 5____\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "\n",
      "--------------Experiment 1--------------\n",
      "43113 10794 2053 514\n",
      "Running RandomForestRegressor\n",
      "Score on training data: 0.9802693511323155\n",
      "Score on testing data: 0.8755775465284016\n",
      "Mean Absolute Error:  0.023514610941344286\n",
      "R Squared:  0.8755775465284016\n",
      "\n",
      "Running GradientBoostingRegressor\n",
      "Score on training data: 0.9540695813207151\n",
      "Score on testing data: 0.9027022323684198\n",
      "Mean Absolute Error:  0.020226535432007156\n",
      "R Squared:  0.9027022323684198\n",
      "\n",
      "Running DecisionTreeRegressor\n",
      "Score on training data: 0.9999998887213692\n",
      "Score on testing data: 0.7559723764912186\n",
      "Mean Absolute Error:  0.026311496403187126\n",
      "R Squared:  0.7559723764912186\n",
      "\n",
      "Running ExtraTreesRegressor\n",
      "Score on training data: 0.9999998887213692\n",
      "Score on testing data: 0.863708524223231\n",
      "Mean Absolute Error:  0.02518152337705599\n",
      "R Squared:  0.863708524223231\n",
      "\n",
      "Running SVR\n",
      "Score on training data: -0.08044109200393224\n",
      "Score on testing data: -0.09296565901862852\n",
      "Mean Absolute Error:  0.12595148640027584\n",
      "R Squared:  -0.09296565901862852\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "\n",
      "____Running all experiments for Region 6____\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "\n",
      "--------------Experiment 1--------------\n",
      "13965 3507 665 167\n",
      "Running RandomForestRegressor\n",
      "Score on training data: 0.976395561649921\n",
      "Score on testing data: 0.8885246030503037\n",
      "Mean Absolute Error:  0.045298246759090915\n",
      "R Squared:  0.8885246030503037\n",
      "\n",
      "Running GradientBoostingRegressor\n",
      "Score on training data: 0.9808861642122295\n",
      "Score on testing data: 0.878846939273491\n",
      "Mean Absolute Error:  0.04650867727192932\n",
      "R Squared:  0.878846939273491\n",
      "\n",
      "Running DecisionTreeRegressor\n",
      "Score on training data: 1.0\n",
      "Score on testing data: 0.8469607081895706\n",
      "Mean Absolute Error:  0.04794594636330755\n",
      "R Squared:  0.8469607081895706\n",
      "\n",
      "Running ExtraTreesRegressor\n",
      "Score on training data: 1.0\n",
      "Score on testing data: 0.8897133679072717\n",
      "Mean Absolute Error:  0.045961648126561894\n",
      "R Squared:  0.8897133679072717\n",
      "\n",
      "Running SVR\n",
      "Score on training data: 0.00815431973774361\n",
      "Score on testing data: -0.01460113301781707\n",
      "Mean Absolute Error:  0.18943113290223365\n",
      "R Squared:  -0.01460113301781707\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "\n",
      "____Running all experiments for Region 8____\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "\n",
      "--------------Experiment 1--------------\n",
      "370755 92694 17655 4414\n",
      "Running RandomForestRegressor\n"
     ]
    }
   ],
   "source": [
    "#regionCount=1\n",
    "regionExperimentDict = {}\n",
    "for i in range(len(dfRegionList)):\n",
    "    dfRegion = dfRegionList[i]\n",
    "    regionCount =i+1\n",
    "    if(regionCount>=7):\n",
    "        regionCount+=1\n",
    "    \n",
    "    experimentList = experimentCreation_FireSizePrediction(dfRegion,\"fire_size\")\n",
    "    print(f\"\\n\\n____Running all experiments for Region {regionCount}____\")\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"____________________________________________\")\n",
    "    experimentListOfDictionaries = []\n",
    "    for i in range(len(experimentList)):\n",
    "        if(i not in [1,2,3]):\n",
    "            experiment=experimentList[i]\n",
    "            print(f\"\\n--------------Experiment {i+1}--------------\")\n",
    "            print(experiment[0].size,experiment[1].size,experiment[2].size,experiment[3].size)\n",
    "            regressorDict = run_all_regressors(experiment[0],experiment[2],experiment[1],experiment[3])\n",
    "            experimentListOfDictionaries.append(regressorDict)\n",
    "    print(\"____________________________________________\")\n",
    "    \n",
    "    regionExperimentDict[regionCount]= experimentListOfDictionaries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    regionCt = i+1\n",
    "    if(regionCt>=7):\n",
    "        regionCt+=1\n",
    "    print(f\"Results from Region {regionCt}\")\n",
    "    print(regionExperimentDict[regionCt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#regionExperimentDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRegionDFResults(regressorDict):\n",
    "    dfRegressionResults = pd.DataFrame(columns=['ModelName','TrainScore','TestScore','MAE','R^2'])\n",
    "    for key in regressorDict.keys():\n",
    "        #print(key)\n",
    "        resultList = regressorDict[key]\n",
    "        new_row = {'ModelName':key, 'TrainScore':resultList[0], 'TestScore':resultList[1], 'MAE':resultList[2] , 'R^2':resultList[3]}\n",
    "        \n",
    "        dfRegressionResults = dfRegressionResults.append(new_row, ignore_index=True)\n",
    "    return dfRegressionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionRegressionList=[]\n",
    "for i in range(1,len(regionExperimentDict)+1):\n",
    "    \n",
    "    if(i>=7):\n",
    "        i+=1\n",
    "    #print(i)\n",
    "    regionRegressionResults = createRegionDFResults(regionExperimentDict[i][0])\n",
    "    regionRegressionList.append(regionRegressionResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(regionRegressionList)):\n",
    "    dfTemp = regionRegressionList[i]\n",
    "    regionNum= i+1\n",
    "    if(regionNum>=7):\n",
    "        regionNum+=1\n",
    "    print(f\"\\nRegion {regionNum}\")\n",
    "    print(dfTemp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO select best model and put in region table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running all Classification Models for Fire Cause Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical values to numeric\n",
    "df['stat_cause_descr'] = df['stat_cause_descr'].apply(lambda x: cause_encoded_dist[x]).astype('int')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionCount=1\n",
    "for dfRegion in dfRegionList:\n",
    "    if(regionCount>=7):\n",
    "        regionCount+=1\n",
    "    \n",
    "    experimentList = experimentCreation_FireSizePrediction(dfRegion)\n",
    "    print(f\"\\n\\n____Running all experiments for Region {regionCount}____\")\n",
    "    print(\"____________________________________________\")\n",
    "    print(\"____________________________________________\")\n",
    "    for i in range(len(experimentList)):\n",
    "        experiment=experimentList[i]\n",
    "        print(f\"\\n--------------Experiment {i+1}--------------\")\n",
    "        print(experiment[0].size,experiment[1].size,experiment[2].size,experiment[3].size)\n",
    "        run_all_regressors(experiment[0],experiment[2],experiment[1],experiment[3])\n",
    "    print(\"____________________________________________\")\n",
    "    regionCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning: TODO Have not done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "search_grid={'n_estimators':[50,100,200],'max_depth':[2,5,8,10]}\n",
    "search=GridSearchCV(estimator=rf_reg,param_grid=search_grid,scoring='neg_mean_absolute_error',n_jobs=1,cv=5, verbose=1)\n",
    "search.fit(df1[0], df1[2])\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor \n",
    "rf_reg = RandomForestRegressor(n_estimators = 200, max_depth=10)\n",
    "\n",
    "# fit the regressor with x and y data\n",
    "rf_reg.fit(df1[0], df1[2])\n",
    "\n",
    "predictions = rf_reg.predict(df1[1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(df1[3], predictions))\n",
    "print('R Squared:', metrics.r2_score(df1[3], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model (from other nb) will run later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting features and target variables\n",
    "X = df[['Vegetation_4','Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "y = df['fire_size']\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #normalizer,\n",
    "    Dense(34, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal',input_dim = X_train.shape[1]),\n",
    "    Dense(34, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01)),\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01)),\n",
    "    Dense(32, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01)),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy','mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mae',\n",
    "        patience=20,\n",
    "        mode='min',\n",
    "        restore_best_weights=True\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_mae = model.evaluate(x=X_test, y=y_test, verbose=0)\n",
    "print('Mean Absolute Error: {acc:0.3f}'.format(acc=test_mae))\n",
    "print('accuracy: {acc:0.3f}'.format(acc=test_acc))\n",
    "print('loss: {acc:0.3f}'.format(acc=test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Fire Size: {df.fire_size.mean()}\")\n",
    "print(f\"Standard Deviation of Fire Size: {df.fire_size.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "df6a2a6d0a1ae49166107b0c1e3b39e060bb8c0d4d18204cd0b91feeca5995f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
